{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1628845722504,
     "user": {
      "displayName": "Yutaka Tak",
      "photoUrl": "",
      "userId": "06686434474661864217"
     },
     "user_tz": -540
    },
    "id": "m2z52g2Yuj6f"
   },
   "outputs": [],
   "source": [
    "# getting access to web and scrape the html using def\n",
    "\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "\"\"\"[Get the list of symbollist (stock ticker symbols) from IBDlog]\n",
    "Returns:\n",
    "  [np array]\n",
    "\"\"\"\n",
    "def get_company_list(url=\"aa\", attrID=\"style\", attrStr=\"border-collapse: collapse;\"):\n",
    "    df = \"\"\n",
    "    print(\"Get company symbols from {}\".format(url))\n",
    "#     req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "#     webpage = urllib.request.urlopen(req).read()  # open web pages\n",
    "#     html = soup(webpage, \"lxml\")  # html = soup(webpage, \"html.parser\")\n",
    "    \n",
    "\n",
    "    req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    webpage = urlopen(req).read()\n",
    "    html = soup(webpage,'lxml')\n",
    "\n",
    "#     print(html)\n",
    "\n",
    "    ibd_table = html.find(\"table\", attrs={attrID: attrStr})  # table data のみにアクセス。\n",
    "    ibd_table_data = ibd_table.find_all(\"tr\")  # access to tr/\n",
    "    # ibd_tablelh_data\n",
    "\n",
    "    getheader = []\n",
    "    for tr in ibd_table_data:\n",
    "        td = tr.find_all(\"td\")\n",
    "        row = [tr.text for tr in td]\n",
    "        getheader.append(row)\n",
    "        # print(getheader)\n",
    "        df = pd.DataFrame(getheader)\n",
    "        df.columns = df.iloc[0]  # Column Name 指定\n",
    "    df\n",
    "\n",
    "    return df\n",
    "\n",
    "'''\n",
    "# headers={'User-Agent': 'Mozilla/5.0'}\n",
    "# response = requests.get('https://www.investors.com/data-tables/top-ranked-low-priced-stocks-Sep-13-2021/', headers=headers)\n",
    "# print(response.status_code)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 2800,
     "status": "ok",
     "timestamp": 1628845734479,
     "user": {
      "displayName": "Yutaka Tak",
      "photoUrl": "",
      "userId": "06686434474661864217"
     },
     "user_tz": -540
    },
    "id": "XjLOU0mD2DJF",
    "outputId": "465ffc48-a404-4469-dabc-9705b880f5bb"
   },
   "outputs": [],
   "source": [
    "# 以下のwebからIBDランキングのデータを取得する。\n",
    "# 日付はいつも最新ではないので、注意して日付を更新する。\n",
    "# ここを走らせれば、symbolの抜粋は可能\n",
    "df_low = get_company_list(\n",
    "    \"https://www.investors.com/data-tables/top-ranked-low-priced-stocks-feb-24-2022/\",attrID=\"class\",attrStr=\"table\",)\n",
    "\n",
    "# print(df_low)\n",
    "# using dictionary to convert specific columns\n",
    "convert_dict = {\"Eps Rtg\": int,\"Rel Str\": float,\"Vol% Chg\": int,\"Cmp Rtg\": int, \"Price Chg\": float, \"Acc Dis\": str,}\n",
    "\n",
    "df_low = df_low[df_low[\"Name\"] != \"Name\"]\n",
    "df_low = df_low.astype(convert_dict)\n",
    "df_low[\"Total\"] = df_low[\"Cmp Rtg\"] + df_low[\"Eps Rtg\"] + df_low[\"Rel Str\"]\n",
    "df_low = df_low[    (df_low[\"Eps Rtg\"] > 60) & (df_low[\"Rel Str\"] > 60) & (df_low[\"Cmp Rtg\"] > 70)]\n",
    "# df_low=df_low.sort_values('Vol% Chg',ascending=False).head(20)\n",
    "# print(df_low.head(10))\n",
    "# df_low.sort_values('Price Chg',ascending=False).head(20)\n",
    "df_low = df_low.sort_values([\"Vol% Chg\", \"Price Chg\", \"Price\"], ascending=[0, 1, 1])\n",
    "\n",
    "# df_low.loc[df_low['Eps Rtg']>90]\n",
    "df_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXV1RYpFwqoQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IBD Large/Mid-Cap Leaders Index\n",
    "\n",
    "url = \"https://www.investors.com/data-tables/ibd-large-mid-cap-leaders-index-apr-29-2021/\"\n",
    "url = \"https://www.investors.com/data-tables/timesaver-table-apr-29-2021/\"\n",
    "url = \"https://www.investors.com/data-tables/dividend-leaders-apr-29-2021/\"\n",
    "url =\"https://www.investors.com/ibd-indexes/ibd-breakout-stocks-index/\"\n",
    "\n",
    "df_largemid = get_company_list(url,attrID=\"class\", attrStr=\"table\",)\n",
    "df_low = df_largemid\n",
    "df_low = df_low.drop(df_low.index[[0]])\n",
    "df_low = df_low.fillna(value=np.nan)\n",
    "for i in df_low['Name']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ij4SaqAcwqoR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_low = pd.read_csv('/Users/yutaka/Downloads/america_2021-05-01.csv').values.tolist()\n",
    "# df_low = pd.read_csv('/Users/yutaka/Downloads/america_2021-05-01.csv')\n",
    "# df_low = df_low.rename({'Ticker': 'Symbol'}, axis=1)  # new method\n",
    "# df_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 146643,
     "status": "ok",
     "timestamp": 1611417541309,
     "user": {
      "displayName": "Yutaka Tak",
      "photoUrl": "",
      "userId": "06686434474661864217"
     },
     "user_tz": 300
    },
    "id": "a3klRb9-BHcz",
    "outputId": "2d1d22f9-9c39-4a71-d7a9-531b7e416098"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "finviz　data download into dataframe.\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 146643,
     "status": "ok",
     "timestamp": 1611417541309,
     "user": {
      "displayName": "Yutaka Tak",
      "photoUrl": "",
      "userId": "06686434474661864217"
     },
     "user_tz": 300
    },
    "id": "a3klRb9-BHcz",
    "outputId": "2d1d22f9-9c39-4a71-d7a9-531b7e416098"
   },
   "outputs": [],
   "source": [
    "end_date = datetime.datetime.now()\n",
    "duration = 1000\n",
    "start_date = end_date - datetime.timedelta(days=duration)\n",
    "\n",
    "###########folder edit#############\n",
    "dir = os.path.join(\"/Users/yutaka/Google ドライブ/New Folder/Python_Output\",    end_date.strftime(\"%y%m%d\") + \"_low-priced\",)\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "# symbol=df_list[0].unique()\n",
    "df = pd.DataFrame()\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "for ind in df_low.index[:]:\n",
    "\n",
    "    i = df_low[\"Symbol\"][ind]  # symbolのindex＃を巡回\n",
    "    # for i in list(ETF_list[:,0]):      ##nparrayで変換しないと動かない。 index の巡回でもいけるはず。\n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", 25)\n",
    "    pd.set_option(\"display.max_rows\", 75)\n",
    "\n",
    "    url = \"http://finviz.com/quote.ashx?t=\" + i.lower()\n",
    "    print(url)\n",
    "    req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    try:\n",
    "        webpage = urlopen(req).read()\n",
    "        html = soup(webpage, \"html.parser\")\n",
    "\n",
    "        def get_fundamentals():\n",
    "            try:\n",
    "                # Find fundamentals table\n",
    "                fundamentals = pd.read_html(str(html), attrs={\"class\": \"snapshot-table2\"})[0]\n",
    "\n",
    "                # Clean up fundamentals dataframe\n",
    "                fundamentals.columns = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\", \"7\",\"8\",\"9\",\"10\",\"11\", ]\n",
    "                colOne = []\n",
    "                colLength = len(fundamentals)\n",
    "                for k in np.arange(0, colLength, 2):\n",
    "                    colOne.append(fundamentals[f\"{k}\"])\n",
    "                attrs = pd.concat(colOne, ignore_index=True)\n",
    "\n",
    "                colTwo = []\n",
    "                colLength = len(fundamentals)\n",
    "                for k in np.arange(1, colLength, 2):\n",
    "                    colTwo.append(fundamentals[f\"{k}\"])\n",
    "                vals = pd.concat(colTwo, ignore_index=True)\n",
    "\n",
    "                fundamentals = pd.DataFrame()\n",
    "                fundamentals[\"Attributes\"] = attrs\n",
    "                fundamentals[\"Values\"] = vals\n",
    "                fundamentals = fundamentals.set_index(\"Attributes\")\n",
    "                return fundamentals\n",
    "\n",
    "            except Exception as e:\n",
    "                return e\n",
    "\n",
    "        df1 = get_fundamentals()\n",
    "    #         print(df1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e.args)\n",
    "\n",
    "    try:\n",
    "        tb10 = yf.download(i, interval=\"1wk\", start=start_date, end=end_date)  # 1wk\n",
    "        dfg = pd.DataFrame(tb10)\n",
    "        dfg = dfg.drop(columns=\"Adj Close\", axis=1)\n",
    "        dfg.columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "        fig, axlist = mpf.plot(\n",
    "            dfg[1:duration],\n",
    "            type=\"candle\",\n",
    "            volume=True,\n",
    "            title=i,\n",
    "            mav=(10, 20, 50, 100, 200),\n",
    "            figratio=(18, 10),\n",
    "            style=\"yahoo\",\n",
    "            savefig=dict(\n",
    "                fname=os.path.join(dir, end_date.strftime(\"%y%m%d_\") + str(i) + \".png\"),\n",
    "                dpi=200,\n",
    "                pad_inches=0.1,\n",
    "            ),\n",
    "            returnfig=True,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Could not download the finance data from Yahoofinance;{}\".format(e.args))\n",
    "        continue\n",
    "\n",
    "    # 全てのdataを\n",
    "    df1 = df1.rename(columns={\"Values\": str(i)})\n",
    "    df = pd.concat([df1, df], axis=1)  # axix=1 means adding columns\n",
    "    # 重複カラム削除\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df.to_csv(os.path.join(dir, end_date.strftime(\"%y%m%d_merge\") + \".csv\"))\n",
    "    df.T.to_csv(os.path.join(dir, end_date.strftime(\"%y%m%d_merge\") + \"T.csv\"))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXjXHC4ewqoU"
   },
   "outputs": [],
   "source": [
    "df_funda = df.T\n",
    "df_funda\n",
    "df_funda.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHvVH-OSwqoZ"
   },
   "outputs": [],
   "source": [
    "#重複カラム削除\n",
    "df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMIkowTQwqoa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert_header = {\"Income\": str, \"EPS Q/Q\": str}\n",
    "df_funda = df_funda.astype(convert_header)\n",
    "df_funda.dtypes\n",
    "\n",
    "df_funda[\"EPS Q/Q\"] = df_funda[\"EPS Q/Q\"].str.replace(\"%\", \"\")\n",
    "df_funda=df_funda[df_funda[\"EPS Q/Q\"] != \"-\"]\n",
    "\n",
    "convert_header = {\"Income\": str, \"EPS Q/Q\": float}\n",
    "df_funda = df_funda.astype(convert_header)\n",
    "df_funda.dtypes\n",
    "\n",
    "# df[\"EPS Q/Q\"]= df[\"EPS Q/Q\"].astype(float)\n",
    "df_funda[[\"Income\",'Sales','Sales Q/Q','EPS Q/Q','Inst Own','Inst Trans','Rel Volume','RSI (14)']].sort_values([\"EPS Q/Q\"], ascending=[0]).head(50)\n",
    "# df_funda.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-iJlDDbwqoa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "\n",
    "url = \"http://finviz.com/quote.ashx?t=RIOT\"  #+ i.lower()\n",
    "print(url)\n",
    "req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "webpage = urlopen(req).read()\n",
    "html = soup(webpage, \"html.parser\")\n",
    "print(html)\n",
    "\n",
    "# def get_fundamentals():\n",
    "#     try:\n",
    "#         # Find fundamentals table\n",
    "#         fundamentals = pd.read_html(str(html), attrs={\"class\": \"snapshot-table2\"})[0]\n",
    "\n",
    "#         # Clean up fundamentals dataframe\n",
    "#         fundamentals.columns = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",]\n",
    "#         colOne = []\n",
    "#         colLength = len(fundamentals)\n",
    "#         for k in np.arange(0, colLength, 2):\n",
    "#             colOne.append(fundamentals[f\"{k}\"])\n",
    "#         attrs = pd.concat(colOne, ignore_index=True)\n",
    "\n",
    "#         colTwo = []\n",
    "#         colLength = len(fundamentals)\n",
    "#         for k in np.arange(1, colLength, 2):\n",
    "#             colTwo.append(fundamentals[f\"{k}\"])\n",
    "#         vals = pd.concat(colTwo, ignore_index=True)\n",
    "\n",
    "#         fundamentals = pd.DataFrame()\n",
    "#         fundamentals[\"Attributes\"] = attrs\n",
    "#         fundamentals[\"Values\"] = vals\n",
    "#         fundamentals = fundamentals.set_index(\"Attributes\")\n",
    "#         return fundamentals\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return e\n",
    "\n",
    "# df1 = get_fundamentals()\n",
    "# #         print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3KF6-2owqob"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.keys import Keys # webdriverからスクレイピングで使用するキーを使えるようにする。\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "DRIVER_PATH = \"/Users/yutaka/Downloads/chromedriver\"\n",
    "\n",
    "def CreatFolder():\n",
    "    dir = os.path.join(\"/Users/yutaka/Google ドライブ/New Folder/Python_Output\",\n",
    "                       end_date.strftime(\"%y%m%d\") + \"_Skew\",)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    return dir\n",
    "\n",
    "# webdriver = webdriver.Chrome()\n",
    "def WebCapture(symbol):\n",
    "    \n",
    "    symbol= symbol.replace(\"/\",\"\")\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//*[@id='siteheader-outer-second']/div/input\").send_keys(symbol)\n",
    "    driver.find_element_by_css_selector(\"input.sym_lookup_btn\").click()\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    screenshot=pyautogui.screenshot(region=(50,50,1500,975))\n",
    "    screenshot.save(CreatFolder() + '/' + symbol+'.png')\n",
    "    driver.find_element_by_xpath(\"//*[@id='siteheader-outer-second']/div/input\").clear()\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH) # , options=options)\n",
    "URL = f'https://marketchameleon.com/Overview/AAPL/VolatilitySkew/'\n",
    "driver.get(URL)\n",
    "\n",
    "# for symb in df_funda.index.values[:]:\n",
    "import pandas as pd\n",
    "Symbol_list = pd.read_csv('/Users/yutaka/Downloads/america_2021-04-24.csv').values.tolist()\n",
    "for ticker in Symbol_list[:]:\n",
    "    WebCapture(ticker[0])\n",
    "\n",
    "# for symb in df_low['Symbol']:\n",
    "#     WebCapture(symb)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLslFx9Pwqoc"
   },
   "outputs": [],
   "source": [
    "for ticker in Symbol_list[:5]:\n",
    "    print(type(ticker[0]))\n",
    "#     WebCapture(ticker[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad6zRCMywqoc"
   },
   "outputs": [],
   "source": [
    "for symb in df_low['Symbol']:\n",
    "    print(type(symb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRFpDhxRwqoc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://au.investing.com/common/modules/js_instrument_chart/api/data.php?pair_id=1&pair_id_for_news=1&chart_type=area&pair_interval=300&candle_count=120&events=yes&volume_series=yes&period='\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:81.0) Gecko/20100101 Firefox/81.0',\n",
    "    'X-Requested-With': 'XMLHttpRequest',\n",
    "    'Referer': 'https://au.investing.com/currencies/eur-usd'}\n",
    "data = requests.get(url, headers=headers).json()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ek_IdDGAAOZt"
   ],
   "name": "00_Top-Ranked Low-Priced Stocks_fromIBD.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
